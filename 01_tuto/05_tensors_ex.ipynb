{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### numpy 기반의 net\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1000)\n",
      "(64, 10)\n",
      "(1000, 100)\n",
      "(100, 10)\n",
      "(64, 100)\n",
      "(64, 100)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, D_in, D_out, H = 64, 1000, 10, 100 # batch size, input_dim, output_dim, hidden_dim\n",
    "\n",
    "# random input and output\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "\n",
    "lr = 1e-6\n",
    "\n",
    "for i in range(500):\n",
    "    # forward\n",
    "    h = x.dot(w1)\n",
    "    print(h.shape)\n",
    "    h_relu = np.maximum(0, h) # relu 구현\n",
    "    print(h_relu.shape)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    \n",
    "    # backprop\n",
    "    grad_y_pred = 2.0*(y_pred-y)\n",
    "    grad_w2 = h_relu\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### torch 기반의 net (without autograd)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 30940392.0\n",
      "199 30940392.0\n",
      "299 30940392.0\n",
      "399 30940392.0\n",
      "499 30940392.0\n",
      "tensor([[   886490.8125,    804029.2500,    883086.2500,  ...,\n",
      "          -1395443.3750, -10250666.0000,  -6932557.0000],\n",
      "        [ -2658750.2500,   1195687.0000,  -1717418.5000,  ...,\n",
      "          -1660828.5000,   4111274.5000,   7753501.0000],\n",
      "        [  -345470.1562,  -2935206.0000,  -2717213.7500,  ...,\n",
      "          -5053170.5000,  -6309516.0000,  -1979812.6250],\n",
      "        ...,\n",
      "        [   203193.4062,   -699593.8125,   3743133.7500,  ...,\n",
      "          -3153885.5000,  -1285123.7500,  -1207944.2500],\n",
      "        [  -698031.1875,   2668137.2500,  -5690574.5000,  ...,\n",
      "           3658498.0000,    844813.0000,   3158208.5000],\n",
      "        [ -1506566.8750,   1194563.8750,  -6570561.0000,  ...,\n",
      "           8516812.0000,   2967491.0000,   2627953.2500]]) torch.Size([1000, 100])\n",
      "tensor([[ 9.8808e+07,  8.7400e+07, -3.7476e+07,  2.8995e+07,  1.1030e+08,\n",
      "          1.0523e+08,  1.1442e+08,  6.0485e+07,  5.3675e+06, -5.1627e+06],\n",
      "        [ 1.0550e+08,  1.3511e+08, -6.5080e+07,  9.3280e+07,  1.4413e+08,\n",
      "          2.0675e+08,  1.0603e+08, -4.2302e+07, -5.7163e+06, -1.2679e+07],\n",
      "        [ 9.7000e+07,  1.1626e+08, -3.8848e+07,  5.8288e+07,  1.6518e+08,\n",
      "          1.2612e+08,  1.3667e+08,  5.5908e+07,  3.3227e+07, -3.7820e+07],\n",
      "        [ 1.4914e+08,  1.7687e+08, -6.2996e+07,  1.2411e+08,  1.5326e+08,\n",
      "          1.1142e+08,  5.7347e+07, -3.5683e+07, -1.3283e+07, -3.5261e+07],\n",
      "        [ 8.3546e+07,  1.3365e+08, -5.5435e+07,  8.2585e+07,  2.0978e+08,\n",
      "          1.5214e+08,  8.5336e+07,  1.7970e+07,  9.1409e+07, -7.1177e+07],\n",
      "        [ 1.2101e+08,  1.4610e+08, -1.0451e+08,  3.4266e+07,  1.0932e+08,\n",
      "          1.1741e+08,  1.5790e+08,  6.5600e+07,  2.2754e+07, -8.3764e+07],\n",
      "        [ 6.1011e+07,  1.0080e+08, -7.4708e+07,  6.8834e+07,  7.8580e+07,\n",
      "          1.4961e+08,  1.1304e+08,  2.7156e+07,  4.0349e+07, -2.2334e+07],\n",
      "        [ 2.3175e+08,  1.7133e+08, -8.7484e+07,  8.1622e+07,  1.4680e+08,\n",
      "          2.0648e+08,  1.8037e+08, -7.9133e+06, -1.1634e+07, -1.4746e+07],\n",
      "        [ 1.4995e+08,  8.7804e+07, -8.6468e+07,  1.0003e+08,  1.1829e+08,\n",
      "          1.9201e+08,  2.2304e+08,  4.0053e+07,  4.5699e+07,  4.2006e+06],\n",
      "        [ 5.4436e+07,  9.7203e+07, -1.2740e+08,  6.2845e+07,  1.4322e+08,\n",
      "          1.9824e+08,  1.1161e+08, -6.2751e+07,  1.0108e+08,  2.1954e+06],\n",
      "        [ 7.6079e+07,  9.6735e+07, -5.0737e+07,  8.8048e+07,  9.0875e+07,\n",
      "          1.3347e+08,  7.9408e+07,  5.2874e+05,  4.9049e+07, -9.4613e+06],\n",
      "        [ 4.1347e+07,  5.2583e+07, -7.3280e+07,  2.5683e+07,  7.1427e+07,\n",
      "          1.2042e+08,  1.0464e+08,  3.7683e+07,  5.4404e+07, -2.8240e+07],\n",
      "        [ 9.4269e+07,  1.1095e+08, -4.6945e+07,  9.7813e+07,  1.1462e+08,\n",
      "          1.8667e+08,  1.4072e+08,  9.8395e+06,  5.0828e+07, -3.4303e+07],\n",
      "        [ 2.1152e+07,  1.1710e+08, -1.1202e+08,  6.4868e+07,  8.5091e+07,\n",
      "          8.5101e+07,  1.0369e+08,  5.5539e+07, -1.9424e+06, -3.7688e+07],\n",
      "        [ 1.0440e+08,  6.6086e+07, -5.7550e+07,  9.2213e+07,  7.3550e+07,\n",
      "          2.1235e+08,  1.2695e+08,  1.7414e+07,  4.4730e+07, -1.1612e+07],\n",
      "        [ 9.0857e+07,  5.9298e+07, -1.0607e+08,  1.0528e+08,  9.6564e+07,\n",
      "          1.6790e+08,  1.0264e+08, -5.5816e+07,  2.4598e+07, -8.6396e+06],\n",
      "        [ 1.3042e+08,  1.4662e+08, -8.6440e+07,  9.8793e+07,  1.3312e+08,\n",
      "          9.3383e+07,  1.0953e+08,  3.4521e+07, -2.4484e+07, -3.7365e+07],\n",
      "        [ 9.6728e+07,  1.1222e+08, -1.2423e+08, -8.6812e+06,  1.2954e+08,\n",
      "          9.4864e+07,  9.9084e+07,  3.1118e+07,  9.9087e+06, -4.3846e+07],\n",
      "        [ 1.6409e+08,  1.4746e+08, -6.1659e+07,  8.3024e+07,  1.4846e+08,\n",
      "          2.1823e+08,  1.6857e+08, -6.7214e+07,  8.6741e+07, -1.8163e+07],\n",
      "        [ 1.9430e+08,  1.0547e+08, -9.1747e+07,  4.8251e+07,  1.2382e+08,\n",
      "          1.8186e+08,  1.7155e+08, -2.3713e+07,  8.4680e+07,  4.9347e+07],\n",
      "        [ 1.8399e+08,  9.1600e+07, -1.2268e+08,  1.0499e+08,  1.4550e+08,\n",
      "          2.3897e+08,  1.7401e+08,  3.0523e+07,  1.1434e+08, -4.3958e+07],\n",
      "        [ 8.9896e+07,  1.2030e+08, -5.2287e+07,  8.5106e+07,  9.9144e+07,\n",
      "          1.5798e+08,  9.0309e+07, -5.5990e+07,  2.4698e+07,  7.9953e+05],\n",
      "        [ 1.2800e+08,  1.7194e+08, -7.2212e+07,  6.0215e+07,  1.2967e+08,\n",
      "          2.2091e+08,  1.4273e+08,  5.7856e+07,  1.2937e+07,  2.7072e+06],\n",
      "        [ 4.9054e+07,  1.1057e+08, -7.7884e+07,  9.4800e+07,  1.0441e+08,\n",
      "          1.6740e+08,  1.4613e+08,  8.6633e+06,  8.0968e+07,  3.5014e+06],\n",
      "        [ 1.2004e+08,  1.3810e+08, -3.0432e+07,  6.7512e+07,  5.0954e+07,\n",
      "          1.2558e+08,  7.3191e+07, -4.9614e+07,  1.8102e+07,  9.8520e+06],\n",
      "        [ 9.9753e+07,  1.5084e+08, -9.0647e+07,  7.5005e+07,  1.3359e+08,\n",
      "          1.6944e+08,  1.7784e+08,  3.0162e+07,  4.0829e+07, -3.4238e+07],\n",
      "        [ 7.9766e+07,  2.0159e+08, -5.7246e+07,  1.3556e+08,  1.7729e+08,\n",
      "          1.7010e+08,  1.4502e+08,  4.0901e+07, -2.2294e+07, -5.0839e+07],\n",
      "        [ 6.8138e+07,  7.7157e+07, -6.3864e+07,  3.1723e+07,  1.0950e+08,\n",
      "          1.1771e+08,  1.0417e+08, -6.4664e+07,  1.3216e+07,  5.7070e+06],\n",
      "        [ 1.1552e+08,  1.1768e+08, -7.1320e+07,  6.5748e+07,  1.2320e+08,\n",
      "          1.4753e+08,  1.0749e+08,  2.5621e+07,  7.0596e+07, -4.4237e+07],\n",
      "        [ 9.0663e+07,  1.0131e+08, -4.6188e+07,  1.2074e+08,  1.0907e+08,\n",
      "          1.6460e+08,  1.1546e+08, -2.2135e+07,  2.7697e+07, -7.7748e+06],\n",
      "        [ 8.6937e+07,  1.0812e+08, -1.0708e+08,  1.2876e+08,  4.9055e+07,\n",
      "          2.2612e+08,  1.5508e+08, -1.2629e+07,  6.6660e+07,  3.0053e+06],\n",
      "        [-2.8395e+07,  1.5290e+08, -9.8985e+07,  4.8440e+07,  1.0530e+08,\n",
      "          1.8482e+08,  1.0792e+08,  8.2743e+06,  3.2061e+07,  1.2593e+06],\n",
      "        [ 1.1861e+08,  7.6918e+07, -6.4552e+07,  5.8428e+07,  1.1994e+08,\n",
      "          1.2132e+08,  1.5191e+08,  6.7424e+07, -1.5092e+07,  2.0360e+07],\n",
      "        [ 1.4955e+08,  1.1434e+08, -6.9393e+07,  1.1298e+08,  7.5112e+07,\n",
      "          1.9854e+08,  7.5816e+07, -8.2645e+06,  3.6710e+07, -4.3095e+06],\n",
      "        [ 1.3880e+08,  8.5240e+07, -9.1684e+07,  8.5044e+07,  1.1988e+08,\n",
      "          1.0340e+08,  8.9216e+07,  1.5587e+07,  5.3352e+06, -3.5063e+07],\n",
      "        [ 1.1444e+08,  1.3805e+08, -6.6616e+07,  1.0406e+08,  1.3020e+08,\n",
      "          1.3992e+08,  9.9703e+07, -1.1250e+07,  7.8747e+07,  1.5064e+07],\n",
      "        [ 1.2447e+08,  1.4050e+08, -4.2191e+07,  1.0382e+08,  1.0492e+08,\n",
      "          1.4506e+08,  8.0000e+07,  7.5506e+06,  4.1685e+07,  1.6284e+07],\n",
      "        [ 1.1214e+08,  1.0141e+08, -1.1421e+08,  1.4039e+08,  1.2766e+08,\n",
      "          2.2163e+08,  1.8740e+08, -5.1413e+07,  1.3123e+08,  9.7547e+06],\n",
      "        [-3.2230e+06,  1.2284e+08, -1.0168e+08,  9.4756e+07,  1.1456e+08,\n",
      "          1.5440e+08,  6.8744e+07, -9.9759e+02,  8.3548e+07, -7.8099e+06],\n",
      "        [ 1.3191e+07,  1.6179e+08, -7.1792e+07,  5.4426e+07,  1.9470e+08,\n",
      "          7.3801e+07,  7.5690e+07,  1.1415e+07, -3.4176e+07, -2.5229e+07],\n",
      "        [ 5.2577e+07,  1.1703e+08, -3.3299e+07,  9.0329e+07,  1.4726e+08,\n",
      "          1.2488e+08,  1.6871e+08,  3.5755e+07,  5.4066e+07, -2.0922e+07],\n",
      "        [ 1.2265e+08,  7.1827e+07, -5.7586e+07,  7.8704e+07,  7.4334e+07,\n",
      "          1.4006e+08,  8.3744e+07, -2.6327e+07,  5.6642e+07, -6.2596e+06],\n",
      "        [ 3.6567e+07,  8.9797e+07, -2.2013e+07,  8.5628e+07,  8.8565e+07,\n",
      "          1.4550e+08,  1.0244e+08,  2.5280e+07,  5.7274e+07,  5.0027e+07],\n",
      "        [ 7.3312e+07,  9.6273e+07, -3.5274e+07,  5.2931e+07,  1.2792e+08,\n",
      "          9.6828e+07,  5.2302e+07,  3.6986e+07,  4.3613e+07, -2.2538e+07],\n",
      "        [ 1.7678e+08,  1.6679e+08, -8.8083e+07,  4.1012e+07,  2.0525e+08,\n",
      "          2.1310e+08,  1.7667e+08,  5.1472e+07,  2.5867e+07, -8.8226e+06],\n",
      "        [ 7.9762e+07,  1.2354e+08, -5.0098e+07,  2.5610e+07,  1.4685e+08,\n",
      "          1.8268e+08,  9.1579e+07, -2.1066e+07,  5.2524e+07, -2.8108e+07],\n",
      "        [ 3.0576e+07,  1.0517e+08, -8.4772e+07,  1.0450e+07,  8.9781e+07,\n",
      "          1.0792e+08,  1.0208e+08,  4.1172e+06, -1.5727e+07,  1.6388e+07],\n",
      "        [ 4.4701e+07,  1.0984e+08, -8.6419e+07,  1.2694e+08,  6.6790e+07,\n",
      "          1.9583e+08,  1.1810e+08, -5.4948e+07,  2.8441e+07,  1.6762e+07],\n",
      "        [ 5.1366e+07,  1.6427e+08, -2.6057e+07,  1.3353e+08,  1.5330e+08,\n",
      "          1.5452e+08,  6.8740e+07,  3.7950e+07,  3.6784e+06,  7.4956e+06],\n",
      "        [ 5.0254e+07,  1.8630e+08, -2.7714e+07,  5.8347e+07,  1.3606e+08,\n",
      "          1.7263e+08,  9.6036e+07, -4.5607e+07,  8.6324e+07,  5.8362e+07],\n",
      "        [ 9.5920e+07,  1.2204e+08, -1.9589e+07,  7.4554e+07,  1.0628e+08,\n",
      "          1.0527e+08,  6.9044e+07, -8.9710e+06,  4.5518e+07,  1.9627e+07],\n",
      "        [ 1.7645e+07,  1.9446e+08, -5.0370e+07,  7.7638e+07,  1.4254e+08,\n",
      "          1.3638e+08,  9.0889e+07, -4.7733e+07,  2.1127e+07, -3.1411e+07],\n",
      "        [ 1.2291e+08,  1.1646e+08, -1.2510e+08,  1.0651e+08,  1.2866e+08,\n",
      "          2.7805e+08,  2.2094e+08,  6.1419e+07,  2.0013e+08, -4.4889e+07],\n",
      "        [ 1.9782e+08,  1.2299e+08, -5.9830e+07,  7.7971e+07,  1.0876e+08,\n",
      "          1.3863e+08,  1.8722e+08, -2.0043e+07,  4.0622e+07, -1.4752e+07],\n",
      "        [ 1.2339e+08,  1.5602e+08, -4.3102e+07,  4.3446e+07,  1.2080e+08,\n",
      "          1.2177e+08,  1.6967e+08, -2.5757e+07,  6.6200e+07, -4.8339e+07],\n",
      "        [ 1.4797e+08,  1.0572e+08, -1.1213e+08,  4.0256e+07,  1.7490e+08,\n",
      "          1.6977e+08,  1.0780e+08,  1.2744e+07,  8.6540e+07, -8.4056e+07],\n",
      "        [ 1.0491e+08,  1.2851e+08, -3.6231e+07,  1.3864e+07,  1.2056e+08,\n",
      "          1.2291e+08,  1.1068e+08,  5.3800e+07,  1.9254e+06,  1.2790e+07],\n",
      "        [ 9.8137e+07,  1.0511e+08, -4.5836e+07,  1.3292e+08,  1.0727e+08,\n",
      "          2.0349e+08,  1.5808e+08, -7.6596e+07,  7.7499e+07,  9.4740e+06],\n",
      "        [ 3.2033e+07,  9.3707e+07, -6.4381e+07,  2.7509e+07,  7.4648e+07,\n",
      "          9.4064e+07,  7.8376e+07, -7.8520e+06,  9.5450e+06, -5.5566e+06],\n",
      "        [ 4.4984e+07,  1.5983e+08, -1.7207e+08,  6.7256e+07,  1.5470e+08,\n",
      "          1.2602e+08,  1.2079e+08,  5.7176e+07,  1.9595e+07, -4.8060e+07],\n",
      "        [ 1.2823e+08,  1.1388e+08, -3.6003e+07,  3.6222e+07,  1.8061e+08,\n",
      "          1.1827e+08,  8.4723e+07,  2.7322e+07, -4.5622e+07, -1.5106e+06],\n",
      "        [ 1.5360e+08,  1.4465e+08, -5.4089e+07,  9.1620e+07,  1.2760e+08,\n",
      "          1.7862e+08,  1.9085e+08, -7.2805e+07,  8.3348e+07, -1.3958e+07],\n",
      "        [ 5.7325e+07,  7.9660e+07, -1.0538e+08,  3.7079e+07,  1.1312e+08,\n",
      "          1.2775e+08,  1.4738e+08,  1.1547e+07,  1.0680e+08, -2.6538e+07],\n",
      "        [ 1.3039e+08,  2.0339e+08, -1.0133e+08,  1.5234e+08,  1.1266e+08,\n",
      "          1.5104e+08,  1.8577e+08, -6.1748e+07,  8.2817e+07, -1.8085e+07],\n",
      "        [ 5.5922e+07,  1.0424e+08, -5.9594e+07,  6.9468e+07,  7.2742e+07,\n",
      "          1.6493e+08,  6.4692e+07, -8.6722e+06,  6.2848e+07, -6.7513e+06],\n",
      "        [ 1.0850e+08,  1.1734e+08, -5.1209e+07,  1.4790e+07,  1.2338e+08,\n",
      "          7.1131e+07,  5.3937e+07,  6.4290e+07,  1.4907e+07,  3.2442e+06],\n",
      "        [ 9.4413e+07,  1.1693e+08, -7.2792e+07,  5.6440e+07,  1.1707e+08,\n",
      "          2.0655e+08,  2.0087e+08,  1.7715e+07,  2.5929e+07, -5.1681e+07],\n",
      "        [ 7.0760e+07,  1.2351e+08, -1.0020e+08,  1.1346e+08,  1.5691e+08,\n",
      "          1.7360e+08,  1.5896e+08,  4.2001e+07,  2.0960e+07, -5.3071e+07],\n",
      "        [ 9.4800e+07,  1.2681e+08, -1.3043e+08,  1.0654e+08,  8.3306e+07,\n",
      "          1.9673e+08,  1.7400e+08, -5.7902e+07,  3.0080e+07, -2.8904e+07],\n",
      "        [ 1.1398e+08,  1.2435e+08, -8.4026e+07,  1.2848e+08,  1.2055e+08,\n",
      "          1.8920e+08,  1.5570e+08,  4.8860e+07,  7.9932e+07, -4.2312e+07],\n",
      "        [ 4.5079e+07,  6.2429e+07, -4.9582e+07,  1.0830e+08,  1.4748e+08,\n",
      "          1.7148e+08,  1.2151e+08, -4.2160e+06,  9.8996e+07, -4.7633e+07],\n",
      "        [ 1.1820e+08,  1.5843e+08, -9.0959e+07,  7.0027e+07,  1.5775e+08,\n",
      "          9.3040e+07,  1.5835e+08,  2.9770e+07,  2.3743e+07, -5.9895e+06],\n",
      "        [ 1.1186e+08,  9.4312e+07, -1.0532e+08,  6.0836e+07,  1.2370e+08,\n",
      "          1.6589e+08,  2.1666e+08, -2.3910e+07,  1.1093e+08, -1.8661e+07],\n",
      "        [ 8.8832e+07,  1.6525e+08, -1.2222e+07,  2.0053e+07,  2.0389e+08,\n",
      "          1.3095e+08,  1.2619e+08,  6.4944e+07,  5.8650e+07,  1.4268e+07],\n",
      "        [ 1.8911e+08,  1.2679e+08, -6.6414e+07,  5.4222e+07,  1.5550e+08,\n",
      "          2.1877e+08,  1.2203e+08,  1.0260e+08,  6.9312e+07,  3.1375e+07],\n",
      "        [ 1.7622e+08,  1.0895e+08, -1.2580e+08,  8.7972e+07,  1.7212e+08,\n",
      "          1.6912e+08,  1.6028e+08, -3.0657e+06,  4.9063e+07,  5.5270e+06],\n",
      "        [ 1.3192e+08,  9.4540e+07, -1.5631e+07,  9.2837e+07,  1.1100e+08,\n",
      "          9.2756e+07,  1.1146e+08,  3.9421e+07,  2.6725e+07, -2.8939e+07],\n",
      "        [ 1.1899e+08,  7.3385e+07, -9.0416e+07,  8.3408e+07,  9.3444e+07,\n",
      "          1.5564e+08,  1.3278e+08, -7.6892e+06,  2.1066e+07, -1.9436e+07],\n",
      "        [ 1.7599e+08,  1.7898e+08, -1.0195e+08,  1.3370e+08,  1.0403e+08,\n",
      "          2.5680e+08,  2.3175e+08, -3.6285e+07,  1.2182e+08, -9.7588e+06],\n",
      "        [ 4.2869e+07,  1.2056e+08, -9.8855e+07,  2.8251e+07,  1.0838e+08,\n",
      "          1.2522e+08,  1.3806e+08, -2.7471e+07,  3.5964e+07,  7.5418e+06],\n",
      "        [ 8.0632e+07,  7.3192e+07, -3.5248e+07,  5.8184e+07,  8.4714e+07,\n",
      "          9.2169e+07,  7.4021e+07,  7.6342e+06,  3.5000e+07, -3.0009e+07],\n",
      "        [ 1.6884e+08,  1.4802e+08, -9.7255e+07,  7.0706e+07,  1.3661e+08,\n",
      "          1.8881e+08,  1.7624e+08, -1.0350e+07,  5.4126e+07, -1.0794e+07],\n",
      "        [ 6.0164e+07,  1.3582e+08, -8.7760e+07,  1.1176e+08,  1.3941e+08,\n",
      "          1.0326e+08,  1.5090e+08, -2.0666e+06,  2.5535e+07, -3.9041e+07],\n",
      "        [ 8.0006e+07,  6.4170e+07, -5.7082e+07,  9.0568e+07,  9.0403e+07,\n",
      "          1.0471e+08,  7.3977e+07,  1.4217e+07,  3.3056e+07, -2.9925e+07],\n",
      "        [ 1.1042e+08,  1.2527e+08, -4.7802e+07,  6.1920e+07,  9.7448e+07,\n",
      "          1.1309e+08,  8.4200e+07, -2.3415e+07, -1.4517e+07, -4.1185e+07],\n",
      "        [ 7.9845e+07,  1.8029e+08, -6.7520e+07,  9.3372e+07,  1.3049e+08,\n",
      "          1.5443e+08,  1.1847e+08, -3.6681e+07,  3.0707e+07, -1.3714e+07],\n",
      "        [ 4.5497e+07,  1.3129e+08, -5.7893e+07,  4.2375e+07,  8.2930e+07,\n",
      "          1.4586e+08,  1.3334e+08, -3.5343e+07,  4.9246e+07,  2.5355e+07],\n",
      "        [ 6.5518e+07,  9.6168e+07, -9.9187e+07,  6.7956e+07,  1.3223e+08,\n",
      "          9.0057e+07,  8.8660e+07,  1.9667e+06,  5.0039e+07, -2.6881e+07],\n",
      "        [ 1.2999e+08,  1.7824e+08, -8.5804e+07,  2.6872e+07,  1.3628e+08,\n",
      "          1.8785e+08,  1.2768e+08,  3.9310e+07,  2.6038e+07, -5.3655e+06],\n",
      "        [ 1.3258e+08,  1.8935e+08, -7.9151e+07,  1.5300e+08,  1.4939e+08,\n",
      "          2.2621e+08,  1.9790e+08,  4.0611e+07,  2.2506e+07,  7.0354e+06],\n",
      "        [ 5.4562e+07,  4.3311e+07, -1.0359e+08,  5.7914e+07,  6.6174e+07,\n",
      "          1.5066e+08,  7.2586e+07,  1.4175e+07,  6.4118e+07, -1.6275e+07],\n",
      "        [ 1.3014e+08,  7.2556e+07, -1.0589e+08,  1.4726e+08,  1.1192e+08,\n",
      "          2.0796e+08,  1.2366e+08, -5.0826e+07,  1.0108e+08,  5.2563e+06],\n",
      "        [ 1.0791e+08,  1.7038e+08, -1.2199e+08,  9.8180e+07,  1.2704e+08,\n",
      "          1.4788e+08,  1.3588e+08,  2.2929e+07, -3.5933e+07, -1.4504e+07],\n",
      "        [ 1.2881e+08,  1.5934e+08, -1.5017e+08,  8.8956e+07,  1.4842e+08,\n",
      "          1.8239e+08,  2.1236e+08, -1.8235e+07,  5.2618e+07, -2.1884e+07],\n",
      "        [ 1.3207e+08,  1.0022e+08, -1.6153e+08,  2.9205e+07,  1.1870e+08,\n",
      "          2.1929e+08,  1.1656e+08, -1.0747e+07,  1.2477e+08, -7.3237e+06],\n",
      "        [ 1.0401e+08,  9.8445e+07, -5.4252e+07,  6.3166e+07,  7.7136e+07,\n",
      "          1.4142e+08,  6.3336e+07, -8.2740e+07,  5.9471e+07,  8.1450e+06],\n",
      "        [ 1.1994e+08,  1.1337e+08, -1.8462e+08,  5.1476e+07,  1.8454e+08,\n",
      "          2.7167e+08,  1.8458e+08, -1.2620e+08,  5.9168e+07,  2.4418e+07],\n",
      "        [ 5.2414e+07,  1.0217e+08, -7.0200e+07,  1.2964e+08,  1.1923e+08,\n",
      "          1.5050e+08,  1.0649e+08,  1.7991e+07,  4.7916e+07, -3.7556e+07],\n",
      "        [ 1.2011e+08,  1.8429e+08, -1.1871e+08,  1.0317e+08,  6.8413e+07,\n",
      "          1.8634e+08,  1.2270e+08, -5.1840e+07, -3.1112e+07,  3.1767e+07],\n",
      "        [ 8.8848e+07,  7.9460e+07, -1.2502e+08,  8.5783e+07,  1.3533e+08,\n",
      "          2.2112e+08,  1.5433e+08, -5.8942e+07,  1.4942e+07, -1.2306e+07]]) torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding : utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:0') # for gpu\n",
    "\n",
    "# N is batch size, D_in in input dimension;\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting require_grad=False indicate that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "# Create random Tensors for weights\n",
    "# Setting require_grad=True indicates that we want to comput gradients with\n",
    "# respect to these Tensors during the backward pass\n",
    "w1 = torch.randn(D_in, H, device = device, dtype = dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass : compute predicted y using operations on Tensors; these\n",
    "    # are exactly the sample operation we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2) # torch.dot의 경우는 1d tensor에 대해 계산. 2d 이상일 경우 torch.mm 이용\n",
    "    \n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() get the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item()) # torch.Tensor.item() -> return to single scalar value\n",
    "    \n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 repectivly.\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "print(w1.grad, w1.grad.size())\n",
    "print(w2.grad, w2.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The backward function receives the gradient of the output Tensors with respect to some scalar value, \n",
    "# and computes the gradient of the input Tensors with respect to that same scalar value\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new w1 and new w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph\n",
    "\n",
    "# nn.Module (hi-level abstraction)\n",
    "## input -> output computation, hold .learnable param\n",
    "## functions such as loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### High-level abstraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(N, D_in, dtype = dtype, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(x.requires_grad) # no_grad()를 해도 requires_grad는 변하지 않음\n",
    "    print((x**2).requires_grad) # 원 tensor에 operation을 적용하는 경우에 대해서만 grad 계산여부 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "99 0.9792891144752502\n",
      "199 0.9679204225540161\n",
      "299 0.9568312764167786\n",
      "399 0.9459875822067261\n",
      "499 0.9353870749473572\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype = dtype)\n",
    "y = torch.randn(N, D_out, dtype = dtype)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    ")\n",
    "print(model)\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 0.0001\n",
    "\n",
    "for i in range(500):\n",
    "    # forward\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if i% 100 == 99:\n",
    "        print(i+1, loss.item())\n",
    "        \n",
    "    # backward\n",
    "    model.zero_grad() # zero-grad before backward\n",
    "    loss.backward() # 이걸 거쳐야 grad 값 계산이 됨\n",
    "    \n",
    "    # grad of first layer\n",
    "    ## list(model.parameters()).grad\n",
    "    \n",
    "    # Manually update weigths using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    \n",
    "    with torch.no_grad(): # don't want to compute the gradients of new weights. just update\n",
    "        for param in model.parameters():\n",
    "            param -= lr*param.grad\n",
    "            \n",
    "    ## torch.no_grad를 하는 이유 -> autograd시에 weight이외 다른 grad를 고려하지 않기 위해\n",
    "            \n",
    "# https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### optimizer function을 이용한 network\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "99 0.9720346927642822\n",
      "199 0.9610562324523926\n",
      "299 0.9502997994422913\n",
      "399 0.9397655725479126\n",
      "499 0.929477870464325\n",
      "599 0.9194071292877197\n",
      "699 0.9095223546028137\n",
      "799 0.8998265266418457\n",
      "899 0.8903145790100098\n",
      "999 0.8809998035430908\n",
      "1099 0.8718146681785583\n",
      "1199 0.8627730011940002\n",
      "1299 0.8539113402366638\n",
      "1399 0.8452159762382507\n",
      "1499 0.8366585969924927\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype = dtype)\n",
    "y = torch.randn(N, D_out, dtype = dtype)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    ")\n",
    "print(model)\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 0.0001\n",
    "\n",
    "opt_fn = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "for i in range(1500):\n",
    "    # forward\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)        \n",
    "\n",
    "    # backward\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers(i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    \n",
    "    opt_fn.zero_grad() # zero-grad before backward\n",
    "    loss.backward() # 이걸 거쳐야 grad 값 계산이 됨\n",
    "    \n",
    "    opt_fn.step()\n",
    "    \n",
    "    if i %100 == 99:\n",
    "        print(i+1, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### zero grad 보기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0025,  0.0038,  0.0052,  ..., -0.0042, -0.0003,  0.0085],\n",
       "        [-0.0042,  0.0019, -0.0017,  ...,  0.0005, -0.0038,  0.0040],\n",
       "        [-0.0080,  0.0076, -0.0032,  ..., -0.0002, -0.0032,  0.0051],\n",
       "        ...,\n",
       "        [ 0.0053, -0.0004,  0.0047,  ..., -0.0007,  0.0027, -0.0033],\n",
       "        [-0.0029,  0.0042,  0.0007,  ..., -0.0032, -0.0002,  0.0009],\n",
       "        [ 0.0001, -0.0050, -0.0031,  ..., -0.0007, -0.0013, -0.0018]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_fn.zero_grad()\n",
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### custom model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customNN(\n",
      "  (ly1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (ly2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "99 0.962489664554596\n",
      "199 0.9513875246047974\n",
      "299 0.9405459761619568\n",
      "399 0.9299857020378113\n",
      "499 0.9196702241897583\n",
      "599 0.9096024036407471\n",
      "699 0.8997359871864319\n",
      "799 0.8900806307792664\n",
      "899 0.8805966973304749\n",
      "999 0.8712757229804993\n",
      "1099 0.86211097240448\n",
      "1199 0.8531219363212585\n",
      "1299 0.844300389289856\n",
      "1399 0.8356345295906067\n",
      "1499 0.8271398544311523\n"
     ]
    }
   ],
   "source": [
    "class customNN(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(customNN, self).__init__()\n",
    "        self.ly1 = nn.Linear(D_in, H)\n",
    "        self.ly2 = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.ly1(x).clamp(min=0)\n",
    "        y_pred = self.ly2(h_relu)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype = dtype)\n",
    "y = torch.randn(N, D_out, dtype = dtype)\n",
    "\n",
    "\n",
    "model = customNN(D_in, H, D_out)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 0.0001\n",
    "\n",
    "opt_fn = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "for i in range(1500):\n",
    "    # forward\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)        \n",
    "\n",
    "    # backward\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers(i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    \n",
    "    opt_fn.zero_grad() # zero-grad before backward\n",
    "    loss.backward() # 이걸 거쳐야 grad 값 계산이 됨\n",
    "    \n",
    "    opt_fn.step()\n",
    "    \n",
    "    if i %100 == 99:\n",
    "        print(i+1, loss.item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## dynamic network\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamicNN(\n",
      "  (input_ly): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (middel_ly): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (output_ly): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "100 1.0636993646621704\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "200 1.0625920295715332\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "300 1.0040253400802612\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "400 0.9791656136512756\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "500 0.9576648473739624\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "600 1.0566067695617676\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "700 1.0554559230804443\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "800 1.05430006980896\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "900 1.0533629655838013\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1000 1.0522336959838867\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class dynamicNN(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(dynamicNN, self).__init__()\n",
    "        \n",
    "        self.input_ly = nn.Linear(D_in, H)\n",
    "        self.middel_ly = nn.Linear(H, H)\n",
    "        self.output_ly = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0,1,2, or 3\n",
    "        and resue the middel_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "        \n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "        \n",
    "        Here we also see that it is perfectly safe to resue the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, when each Module could be used only once.\n",
    "        \"\"\"\n",
    "        \n",
    "        h_relu = self.input_ly(x).clamp(min=0)\n",
    "        for i in range(random.randint(0,3)):\n",
    "            print(i)\n",
    "            h_relu = self.middel_ly(h_relu).clamp(min=0)  # model(x) 할 때 마다 middle_ly 개수가 달라짐 (처음 객체선언 한번이 아니라)\n",
    "        y_pred = self.output_ly(h_relu)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "model = dynamicNN(D_in, H, D_out)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 1e-3\n",
    "\n",
    "opt_fn = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    opt_fn.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_fn.step()\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(i + 1, loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
