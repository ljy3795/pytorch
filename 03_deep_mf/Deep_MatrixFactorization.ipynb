{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Python library load\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:46.138976Z",
     "start_time": "2018-12-24T15:27:46.133431Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:47.365628Z",
     "start_time": "2018-12-24T15:27:46.141711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Current path is /home/ubuntu/notebooks/pytorch/03_deep_mf\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "## Python library Load \n",
    "#------------------------------------------------------------\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "import logging, copy, time, pickle, os\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# pytorch module load\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "# data / models / images dir생성\n",
    "print(\"Current path is {}\".format(os.getcwd()))\n",
    "    \n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'models')):\n",
    "    os.makedirs(os.path.join(os.getcwd(), 'models'))\n",
    "    \n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'images')):\n",
    "    os.makedirs(os.path.join(os.getcwd(), 'images'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T08:42:26.538328Z",
     "start_time": "2018-12-22T08:42:26.534821Z"
    }
   },
   "source": [
    "---\n",
    "### Contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Building Matrix Factorization with Pytorch\n",
    "- 2) Building Deep Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 0) Load the movielens dataset and simple EDA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:50.488350Z",
     "start_time": "2018-12-24T15:27:47.367804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded filesize is 0.93 mb\n",
      "downloaded files : {'ml-latest-small.zip', 'ratings.csv', 'movies.csv'}\n"
     ]
    }
   ],
   "source": [
    "from utils.datasets import movielens\n",
    "dataset_nm = 'ml-latest-small'\n",
    "data_dir = 'dataset_down'\n",
    "movielens.download_unzip(dataset_nm, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:50.979276Z",
     "start_time": "2018-12-24T15:27:50.490464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userID  movieID  ratings\n",
      "0       1        1      4.0\n",
      "1       1        3      4.0\n",
      "2       1        6      4.0\n",
      "3       1       47      5.0\n",
      "4       1       50      5.0\n",
      "(100836, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./{}/{}/ratings.csv'.format(data_dir,dataset_nm),sep=\",\", engine = 'python', usecols=(0, 1, 2))\n",
    "data.columns = ['userID','movieID','ratings']\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:50.986349Z",
     "start_time": "2018-12-24T15:27:50.981309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value of rating : 0.5\n",
      "Max value of rating : 5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Min value of rating : {}\".format(data.ratings.min(axis=0)))\n",
    "print(\"Max value of rating : {}\".format(data.ratings.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.155970Z",
     "start_time": "2018-12-24T15:27:50.988366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(facecolor='w')\n",
    "plt.hist(data['ratings'])\n",
    "plt.xlabel(\"Rating\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.title(\"Distribution of Ratings in MovieLens\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.173758Z",
     "start_time": "2018-12-24T15:27:51.158301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id min/max:  1 610\n",
      "# unique users:  610\n",
      "\n",
      "movie id min/max:  1 193609\n",
      "# unique movies:  9724\n"
     ]
    }
   ],
   "source": [
    "print(\"user id min/max: \", data['userID'].min(), data['userID'].max())\n",
    "print(\"# unique users: \", numpy.unique(data['userID']).shape[0])\n",
    "print(\"\")\n",
    "print(\"movie id min/max: \", data['movieID'].min(), data['movieID'].max())\n",
    "print(\"# unique movies: \", numpy.unique(data['movieID']).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.194339Z",
     "start_time": "2018-12-24T15:27:51.176313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>movieID</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76</td>\n",
       "      <td>81535</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>364</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>534</td>\n",
       "      <td>101864</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>1645</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>608</td>\n",
       "      <td>2167</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  movieID  ratings\n",
       "0      76    81535      4.5\n",
       "1      39      364      4.0\n",
       "2     534   101864      4.0\n",
       "3      64     1645      2.5\n",
       "4     608     2167      4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random shuffling\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T01:54:58.950233Z",
     "start_time": "2018-12-24T01:54:58.945881Z"
    }
   },
   "source": [
    "---\n",
    "### 1) Building Matrix Factorization with PyTorch\n",
    "- building matrix factorization model using PyTorch\n",
    "- that is, used embedding layer to build latent variable and dot product to compute recomm. scores by PyTorch operation\n",
    "- PyTorch building\n",
    "  * Latent Variable -> torch.nn.Embedding() Layer\n",
    "  * dot Product -> torch.sum(user_embedding_values  item_embedding_values*, 1),  matrix muplication with column-wise summation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (0) preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.304726Z",
     "start_time": "2018-12-24T15:27:51.196977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 2)\n",
      "(9724, 2)\n"
     ]
    }
   ],
   "source": [
    "users = data['userID'].tolist()\n",
    "movies = data['movieID'].tolist()\n",
    "ratings = data['ratings'].tolist()\n",
    "\n",
    "# unique index for users and movies (to be used in embedding layers)\n",
    "users_ind = data['userID'].astype('category').cat.codes.tolist()\n",
    "movies_ind = data['movieID'].astype('category').cat.codes.tolist()\n",
    "\n",
    "user_ind_mapping = pd.DataFrame({'userID':users, 'userID_ind':users_ind}).drop_duplicates()\n",
    "movie_ind_mapping = pd.DataFrame({'movieID':movies, 'movieID_ind':movies_ind}).drop_duplicates()\n",
    "\n",
    "print(user_ind_mapping.shape)\n",
    "print(movie_ind_mapping.shape)\n",
    "\n",
    "n_users, n_movies = len(set(users)), len(set(movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.318260Z",
     "start_time": "2018-12-24T15:27:51.306794Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the size of embeddling layers input\n",
    "train_size = int(len(users)*.9) # 90% train_set, 10% valid_set\n",
    "batch_size = 15000\n",
    "\n",
    "# split test and train data set (100,000 for test vs remaining for training)\n",
    "train_dataset = data_utils.TensorDataset(torch.tensor(users_ind[train_size:], dtype = torch.long), torch.tensor(movies_ind[train_size:], dtype = torch.long), torch.tensor(ratings[train_size:], dtype = torch.float32))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=30)\n",
    "\n",
    "test_dataset = data_utils.TensorDataset(torch.tensor(users_ind[:train_size], dtype = torch.long), torch.tensor(movies_ind[:train_size], dtype = torch.long), torch.tensor(ratings[:train_size], dtype = torch.float32))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=True, num_workers=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.344683Z",
     "start_time": "2018-12-24T15:27:51.320290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# set gpu mode\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) build (Vanilla) Matrix Factorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.352271Z",
     "start_time": "2018-12-24T15:27:51.346993Z"
    }
   },
   "outputs": [],
   "source": [
    "class Vanilla_MF(nn.Module):\n",
    "    def __init__(self, user_n, item_n, latent_dim):\n",
    "        super(Vanilla_MF, self).__init__()\n",
    "        \n",
    "        self.user = nn.Embedding(num_embeddings=user_n, embedding_dim=latent_dim) # user dim x embedding dim\n",
    "        self.item = nn.Embedding(num_embeddings=item_n, embedding_dim=latent_dim) # output dim x embedding dim\n",
    "        \n",
    "    def forward(self, user_i, item_i):\n",
    "        # embedding values of user & item\n",
    "        user_e = self.user(user_i)\n",
    "        item_e = self.item(item_i)\n",
    "        \n",
    "        # rating value of user_i & item_i\n",
    "        pred = torch.sum(user_e*item_e, 1) # prediction values\n",
    "        reg_loss = torch.sum(torch.sum(user_e*user_e,1)) + torch.sum(torch.sum(item_e*item_e,1)) # regularization l2 losses\n",
    "        \n",
    "        return pred, reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss function (ref. http://yifanhu.net/PUB/cf.pdf)\n",
    "\n",
    "<img src=\"./images/loss_fn_1.png\" width=\"400\" align=\"left\"/>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:51.357617Z",
     "start_time": "2018-12-24T15:27:51.354014Z"
    }
   },
   "outputs": [],
   "source": [
    "## Input Args.\n",
    "n_users = n_users\n",
    "n_items = n_movies\n",
    "n_factor = 25 # dimension of latent variable \n",
    "\n",
    "reg_v = 0.3\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:53.947811Z",
     "start_time": "2018-12-24T15:27:51.359532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vanilla_MF(\n",
       "  (user): Embedding(610, 25)\n",
       "  (item): Embedding(9724, 25)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model build\n",
    "model = Vanilla_MF(n_users, n_items, n_factor)\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:53.953160Z",
     "start_time": "2018-12-24T15:27:53.949774Z"
    }
   },
   "outputs": [],
   "source": [
    " # Loss function & Optimizer\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "opt_fn = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:27:53.964982Z",
     "start_time": "2018-12-24T15:27:53.955223Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, reg_v, num_epochs = 30):\n",
    "    print(\"Running on {}\".format(device))\n",
    "    print(\"batch size is {:,}\".format(train_loader.batch_size))\n",
    "    start_t = time.time()\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set model to training mode\n",
    "        \n",
    "        total_running_loss = 0.0\n",
    "        mse_running_loss = 0.0\n",
    "        \n",
    "        # for training\n",
    "        i = 0\n",
    "        iter_cnt = 0\n",
    "        for user, item, rating in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            rating = rating.to(device)\n",
    "            \n",
    "            batch_size = user.size(0)\n",
    "\n",
    "            # (0) zero grads\n",
    "            opt_fn.zero_grad()\n",
    "\n",
    "            # (1) forward + loss calculation\n",
    "            preds, reg_loss = model(user, item)\n",
    "            mse_loss = mse_loss_fn(preds.squeeze(), rating)\n",
    "            loss = mse_loss + reg_v*reg_loss/batch_size\n",
    "            \n",
    "            # (2) backward + optimize\n",
    "            loss.backward()  # calculate gradients with embedded autograd fn of pytorch\n",
    "            for i in range(user.size(0)):\n",
    "                opt_fn.step()\n",
    "\n",
    "            # result(loss) summary\n",
    "            total_running_loss += loss.item()*batch_size # loss.item() is the mean of total losses of mini-batch. So, multiply mini-batch\n",
    "            mse_running_loss += mse_loss.item()*batch_size\n",
    "            i += 1\n",
    "            iter_cnt += batch_size\n",
    "            \n",
    "        epoch_loss = total_running_loss / iter_cnt     \n",
    "        epoch_mse_loss = mse_running_loss / iter_cnt     \n",
    "        train_loss.append((epoch, epoch_loss, epoch_mse_loss))\n",
    "        \n",
    "        \n",
    "        # for validation        \n",
    "        model.eval()\n",
    "        mse_running_loss = 0.0        \n",
    "        \n",
    "        i = 0\n",
    "        iter_cnt = 0\n",
    "        for user, item, rating in test_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            rating = rating.to(device)           \n",
    "            \n",
    "            batch_size = user.size(0)\n",
    "            \n",
    "            # (1) forward + loss calculation\n",
    "            with torch.no_grad():\n",
    "#                 preds, reg_loss = model(user, item)\n",
    "                preds, _ = model(user, item)\n",
    "                loss = mse_loss_fn(preds.squeeze(), rating)\n",
    "            \n",
    "            # result(loss) summary\n",
    "            mse_running_loss += loss.item()*batch_size # loss.item() is the mean of total losses of mini-batch. So, multiply mini-batch\n",
    "            i += 1\n",
    "            iter_cnt += batch_size\n",
    "            \n",
    "        mse_epoch_loss = mse_running_loss / iter_cnt     \n",
    "        test_loss.append((epoch, mse_epoch_loss))\n",
    "            \n",
    "        print('Epoch {} / {} [Train] Total loss {:.4f}, MSE Loss: {:.4f}, [Test] MSE Loss: {:.4f}, running time : {:.1f} sec '.format(epoch + 1, num_epochs, train_loss[epoch][1], train_loss[epoch][2], test_loss[epoch][1], time.time()-start_t))\n",
    "        print('-'*20)\n",
    "        \n",
    "    print(\"total elapsed time is {:.1f}sec\".format(time.time()-start_t))\n",
    "    torch.cuda.empty_cache()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:29:22.623720Z",
     "start_time": "2018-12-24T15:27:53.967424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n",
      "batch size is 15,000\n",
      "Epoch 1 / 20 [Train] Total loss 52.1385, MSE Loss: 37.4107, [Test] MSE Loss: 13.8632, running time : 4.5 sec \n",
      "--------------------\n",
      "Epoch 2 / 20 [Train] Total loss 16.2265, MSE Loss: 13.9709, [Test] MSE Loss: 13.4297, running time : 9.0 sec \n",
      "--------------------\n",
      "Epoch 3 / 20 [Train] Total loss 14.4422, MSE Loss: 13.5175, [Test] MSE Loss: 13.3551, running time : 13.5 sec \n",
      "--------------------\n",
      "Epoch 4 / 20 [Train] Total loss 13.7416, MSE Loss: 13.4079, [Test] MSE Loss: 13.3445, running time : 17.9 sec \n",
      "--------------------\n",
      "Epoch 5 / 20 [Train] Total loss 13.5402, MSE Loss: 13.3901, [Test] MSE Loss: 13.3431, running time : 22.4 sec \n",
      "--------------------\n",
      "Epoch 6 / 20 [Train] Total loss 13.4630, MSE Loss: 13.3912, [Test] MSE Loss: 13.3434, running time : 26.8 sec \n",
      "--------------------\n",
      "Epoch 7 / 20 [Train] Total loss 13.4336, MSE Loss: 13.3884, [Test] MSE Loss: 13.3433, running time : 31.3 sec \n",
      "--------------------\n",
      "Epoch 8 / 20 [Train] Total loss 13.4211, MSE Loss: 13.3903, [Test] MSE Loss: 13.3431, running time : 35.7 sec \n",
      "--------------------\n",
      "Epoch 9 / 20 [Train] Total loss 13.4176, MSE Loss: 13.3901, [Test] MSE Loss: 13.3432, running time : 40.2 sec \n",
      "--------------------\n",
      "Epoch 10 / 20 [Train] Total loss 13.4149, MSE Loss: 13.3891, [Test] MSE Loss: 13.3436, running time : 44.7 sec \n",
      "--------------------\n",
      "Epoch 11 / 20 [Train] Total loss 13.4134, MSE Loss: 13.3896, [Test] MSE Loss: 13.3430, running time : 49.1 sec \n",
      "--------------------\n",
      "Epoch 12 / 20 [Train] Total loss 13.4102, MSE Loss: 13.3890, [Test] MSE Loss: 13.3433, running time : 53.4 sec \n",
      "--------------------\n",
      "Epoch 13 / 20 [Train] Total loss 13.4054, MSE Loss: 13.3879, [Test] MSE Loss: 13.3430, running time : 57.9 sec \n",
      "--------------------\n",
      "Epoch 14 / 20 [Train] Total loss 13.4041, MSE Loss: 13.3882, [Test] MSE Loss: 13.3432, running time : 62.2 sec \n",
      "--------------------\n",
      "Epoch 15 / 20 [Train] Total loss 13.4027, MSE Loss: 13.3882, [Test] MSE Loss: 13.3432, running time : 66.7 sec \n",
      "--------------------\n",
      "Epoch 16 / 20 [Train] Total loss 13.4026, MSE Loss: 13.3888, [Test] MSE Loss: 13.3432, running time : 71.0 sec \n",
      "--------------------\n",
      "Epoch 17 / 20 [Train] Total loss 13.4023, MSE Loss: 13.3887, [Test] MSE Loss: 13.3432, running time : 75.4 sec \n",
      "--------------------\n",
      "Epoch 18 / 20 [Train] Total loss 13.4027, MSE Loss: 13.3891, [Test] MSE Loss: 13.3433, running time : 79.8 sec \n",
      "--------------------\n",
      "Epoch 19 / 20 [Train] Total loss 13.4021, MSE Loss: 13.3884, [Test] MSE Loss: 13.3432, running time : 84.3 sec \n",
      "--------------------\n",
      "Epoch 20 / 20 [Train] Total loss 13.4025, MSE Loss: 13.3892, [Test] MSE Loss: 13.3432, running time : 88.6 sec \n",
      "--------------------\n",
      "total elapsed time is 88.7sec\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader, test_loader, reg_v, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:29:22.632840Z",
     "start_time": "2018-12-24T15:29:22.626574Z"
    }
   },
   "outputs": [],
   "source": [
    "# valinla MF with bias\n",
    "class Vanilla_MF_w_bias(nn.Module):\n",
    "    def __init__(self, user_n, item_n, user_latent_n, item_latent_n, mean_v):\n",
    "        super(Vanilla_MF_w_bias, self).__init__()\n",
    "        \n",
    "        self.user = nn.Embedding(num_embeddings=user_n, embedding_dim=user_latent_n) # user dim x embedding dim\n",
    "        self.user_bias = nn.Embedding(num_embeddings=user_n, embedding_dim=1)\n",
    "        \n",
    "        self.item = nn.Embedding(num_embeddings=item_n, embedding_dim=item_latent_n) # output dim x embedding dim\n",
    "        self.item_bias = nn.Embedding(num_embeddings=item_n, embedding_dim=1) # output dim x embedding dim\n",
    "        \n",
    "        self.mean_v = mean_v\n",
    "        \n",
    "    def forward(self, user_i, item_i):\n",
    "        # embedding values of user & item\n",
    "        user_e = self.user(user_i)\n",
    "        user_b = self.user_bias(user_i).squeeze()\n",
    "        \n",
    "        item_e = self.item(item_i)\n",
    "        item_b = self.item_bias(item_i).squeeze()\n",
    "        \n",
    "        # rating value of user_i & item_i\n",
    "        preds = torch.sum(user_e*item_e, 1)\n",
    "        preds = preds + user_b + item_b\n",
    "        preds = preds + self.mean_v\n",
    "        \n",
    "        reg_loss = torch.sum(torch.sum(user_e*user_e,1)) + torch.sum(torch.sum(item_e*item_e,1)) # l2 losses\n",
    "        \n",
    "        return preds, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:29:22.648955Z",
     "start_time": "2018-12-24T15:29:22.635335Z"
    }
   },
   "outputs": [],
   "source": [
    "## Input Args.\n",
    "n_users = n_users\n",
    "n_items = n_movies\n",
    "n_factor = 25 # dimentions of latent variable\n",
    "reg_v = 0.3\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 전체 스코어 평균값 계산\n",
    "mean_v = torch.tensor(np.mean(ratings[train_size:])).to(device)\n",
    "mean_v = np.mean(ratings)\n",
    "\n",
    "model = Vanilla_MF_w_bias(n_users, n_items, n_factor, n_factor, mean_v)\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:29:22.653422Z",
     "start_time": "2018-12-24T15:29:22.650736Z"
    }
   },
   "outputs": [],
   "source": [
    "## params\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "opt_fn = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:31:37.152676Z",
     "start_time": "2018-12-24T15:29:22.655555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n",
      "batch size is 15,000\n",
      "Epoch 1 / 20 [Train] Total loss 43.0318, MSE Loss: 27.9549, [Test] MSE Loss: 1.8935, running time : 6.6 sec \n",
      "--------------------\n",
      "Epoch 2 / 20 [Train] Total loss 4.2412, MSE Loss: 1.9372, [Test] MSE Loss: 1.3029, running time : 13.4 sec \n",
      "--------------------\n",
      "Epoch 3 / 20 [Train] Total loss 2.2217, MSE Loss: 1.3163, [Test] MSE Loss: 1.1438, running time : 20.0 sec \n",
      "--------------------\n",
      "Epoch 4 / 20 [Train] Total loss 1.4373, MSE Loss: 1.1374, [Test] MSE Loss: 1.1320, running time : 26.8 sec \n",
      "--------------------\n",
      "Epoch 5 / 20 [Train] Total loss 1.2548, MSE Loss: 1.1283, [Test] MSE Loss: 1.1188, running time : 33.7 sec \n",
      "--------------------\n",
      "Epoch 6 / 20 [Train] Total loss 1.1704, MSE Loss: 1.1074, [Test] MSE Loss: 1.1246, running time : 40.5 sec \n",
      "--------------------\n",
      "Epoch 7 / 20 [Train] Total loss 1.1595, MSE Loss: 1.1195, [Test] MSE Loss: 1.1125, running time : 47.1 sec \n",
      "--------------------\n",
      "Epoch 8 / 20 [Train] Total loss 1.1272, MSE Loss: 1.0981, [Test] MSE Loss: 1.1187, running time : 53.9 sec \n",
      "--------------------\n",
      "Epoch 9 / 20 [Train] Total loss 1.1375, MSE Loss: 1.1130, [Test] MSE Loss: 1.1216, running time : 60.5 sec \n",
      "--------------------\n",
      "Epoch 10 / 20 [Train] Total loss 1.1260, MSE Loss: 1.1057, [Test] MSE Loss: 1.1220, running time : 67.2 sec \n",
      "--------------------\n",
      "Epoch 11 / 20 [Train] Total loss 1.1298, MSE Loss: 1.1146, [Test] MSE Loss: 1.1220, running time : 73.9 sec \n",
      "--------------------\n",
      "Epoch 12 / 20 [Train] Total loss 1.1229, MSE Loss: 1.1101, [Test] MSE Loss: 1.1165, running time : 80.6 sec \n",
      "--------------------\n",
      "Epoch 13 / 20 [Train] Total loss 1.1243, MSE Loss: 1.1115, [Test] MSE Loss: 1.1244, running time : 87.3 sec \n",
      "--------------------\n",
      "Epoch 14 / 20 [Train] Total loss 1.1188, MSE Loss: 1.1071, [Test] MSE Loss: 1.1148, running time : 93.9 sec \n",
      "--------------------\n",
      "Epoch 15 / 20 [Train] Total loss 1.1201, MSE Loss: 1.1087, [Test] MSE Loss: 1.1210, running time : 100.6 sec \n",
      "--------------------\n",
      "Epoch 16 / 20 [Train] Total loss 1.1179, MSE Loss: 1.1065, [Test] MSE Loss: 1.1216, running time : 107.4 sec \n",
      "--------------------\n",
      "Epoch 17 / 20 [Train] Total loss 1.1254, MSE Loss: 1.1135, [Test] MSE Loss: 1.1229, running time : 114.2 sec \n",
      "--------------------\n",
      "Epoch 18 / 20 [Train] Total loss 1.1181, MSE Loss: 1.1063, [Test] MSE Loss: 1.1219, running time : 121.0 sec \n",
      "--------------------\n",
      "Epoch 19 / 20 [Train] Total loss 1.1276, MSE Loss: 1.1159, [Test] MSE Loss: 1.1185, running time : 127.8 sec \n",
      "--------------------\n",
      "Epoch 20 / 20 [Train] Total loss 1.1118, MSE Loss: 1.1003, [Test] MSE Loss: 1.1199, running time : 134.5 sec \n",
      "--------------------\n",
      "total elapsed time is 134.5sec\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader, test_loader, reg_v, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2) Building Deep Matrix Factorization\n",
    "- To build the relationship of Users and Moview in a non-linear model, Used the Neural Network(NN) architecture\n",
    "- For buildng deep Matrix Facotrization using PyTorch\n",
    "  * Latent Variable -> torch.nn.Embedding() Layer\n",
    "  * forward & backword path between NN layers could be built on non-linear activation functions like sigmoid / relu\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:31:37.162446Z",
     "start_time": "2018-12-24T15:31:37.155385Z"
    }
   },
   "outputs": [],
   "source": [
    "class deep_MF(nn.Module):\n",
    "    def __init__(self, user_n, item_n, emb_d1, emb_d2):\n",
    "        super(deep_MF, self).__init__()        \n",
    "        # params\n",
    "        self.user_n = int(user_n)\n",
    "        self.item_n = int(item_n)\n",
    "        self.emb_d1 = int(emb_d1)    \n",
    "        self.emb_d2 = int(emb_d2)      \n",
    "        \n",
    "        # embedding layers\n",
    "        self.user_ly = nn.Embedding(num_embeddings=user_n, embedding_dim=emb_d1) # input dim x embedding dim (25 latent vars)\n",
    "        self.item_ly = nn.Embedding(num_embeddings=item_n, embedding_dim=emb_d2) # output dim x embedding dim (25 latent vars)\n",
    "        \n",
    "        # fully connected-layers\n",
    "        self.fc1 = nn.Linear((self.emb_d1 + self.emb_d2), 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(64)\n",
    "        \n",
    "        \n",
    "    def forward(self, user_i, item_i):\n",
    "        # user & item embeddings \n",
    "        user_e = self.user_ly(user_i)\n",
    "        item_e = self.item_ly(item_i)\n",
    "        \n",
    "        # cocnat two embedding vectors and flatten\n",
    "        concat_t = torch.cat((user_e, item_e),1)\n",
    "        flat_t = concat_t.view(concat_t.size(0),-1)\n",
    "        \n",
    "        # add fully-connected layers\n",
    "        out_fc1 = self.batch_norm(F.relu(self.fc1(flat_t)))\n",
    "        out_fc2 = self.batch_norm(F.relu(self.fc2(out_fc1)))\n",
    "        out = self.fc3(out_fc2)\n",
    "        \n",
    "        return out, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:31:37.175753Z",
     "start_time": "2018-12-24T15:31:37.164737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deep_MF(\n",
       "  (user_ly): Embedding(610, 25)\n",
       "  (item_ly): Embedding(9724, 25)\n",
       "  (fc1): Linear(in_features=50, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input Args.\n",
    "n_users = n_users\n",
    "n_items = n_movies\n",
    "n_factor = 25\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = deep_MF(n_users, n_items, n_factor, n_factor)\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:31:37.180601Z",
     "start_time": "2018-12-24T15:31:37.177653Z"
    }
   },
   "outputs": [],
   "source": [
    "## params\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "opt_fn = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:31:38.178887Z",
     "start_time": "2018-12-24T15:31:38.170027Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs = 30):\n",
    "    print(\"Running on {}\".format(device))\n",
    "    print(\"batch size is {:,}\".format(train_loader.batch_size))\n",
    "    start_t = time.time()\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # for training\n",
    "        i = 0\n",
    "        iter_cnt = 0\n",
    "        for user, item, rating in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            rating = rating.to(device)\n",
    "            \n",
    "            batch_size = user.size(0)\n",
    "\n",
    "            # (0) zero grads\n",
    "            opt_fn.zero_grad()\n",
    "\n",
    "            # (1) forward + loss calculation\n",
    "            preds, _ = model(user, item)\n",
    "            loss = mse_loss_fn(preds.squeeze(), rating)\n",
    "\n",
    "            # (2) backward + optimize\n",
    "            loss.backward()  # calculate gradients with embedded autograd fn of pytorch\n",
    "            for i in range(10):\n",
    "                opt_fn.step()\n",
    "\n",
    "            # result(loss) summary\n",
    "            running_loss += loss.item()*batch_size # loss.item() is the mean of total losses of mini-batch. So, multiply mini-batch\n",
    "            i += 1\n",
    "            iter_cnt += batch_size\n",
    "            \n",
    "        epoch_loss = running_loss / iter_cnt     \n",
    "        train_loss.append((epoch, epoch_loss, 'loss'))\n",
    "        \n",
    "        \n",
    "        # for validation        \n",
    "        model.eval()\n",
    "        running_loss = 0.0        \n",
    "        \n",
    "        i = 0\n",
    "        iter_cnt = 0\n",
    "        for user, item, rating in test_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            rating = rating.to(device)           \n",
    "            \n",
    "            batch_size = user.size(0)\n",
    "            \n",
    "            # (1) forward + loss calculation\n",
    "            with torch.no_grad():\n",
    "                preds, _ = model(user, item)\n",
    "                loss = mse_loss_fn(preds.squeeze(), rating)\n",
    "            \n",
    "            # result(loss) summary\n",
    "            running_loss += loss.item()*batch_size # loss.item() is the mean of total losses of mini-batch. So, multiply mini-batch\n",
    "            i += 1\n",
    "            iter_cnt += batch_size\n",
    "            \n",
    "        epoch_loss = running_loss / iter_cnt     \n",
    "        test_loss.append((epoch, epoch_loss, 'loss'))\n",
    "            \n",
    "        print('Epoch {} / {} [Train] MSE Loss: {:.4f}, [Test] MSE Loss: {:.4f}, running time : {:.1f} sec '.format(epoch + 1, num_epochs, train_loss[epoch][1],test_loss[epoch][1], time.time()-start_t))\n",
    "        print('-'*20)\n",
    "        \n",
    "    print(\"total elapsed time is {:.1f}sec\".format(time.time()-start_t))\n",
    "    torch.cuda.empty_cache()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:32:47.937945Z",
     "start_time": "2018-12-24T15:31:38.180769Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n",
      "batch size is 15,000\n",
      "Epoch 1 / 30 [Train] MSE Loss: 14.1389, [Test] MSE Loss: 13.7342, running time : 2.3 sec \n",
      "--------------------\n",
      "Epoch 2 / 30 [Train] MSE Loss: 13.7006, [Test] MSE Loss: 13.4318, running time : 4.6 sec \n",
      "--------------------\n",
      "Epoch 3 / 30 [Train] MSE Loss: 13.3001, [Test] MSE Loss: 13.0488, running time : 6.9 sec \n",
      "--------------------\n",
      "Epoch 4 / 30 [Train] MSE Loss: 12.7751, [Test] MSE Loss: 12.4966, running time : 9.3 sec \n",
      "--------------------\n",
      "Epoch 5 / 30 [Train] MSE Loss: 12.1159, [Test] MSE Loss: 11.7389, running time : 11.5 sec \n",
      "--------------------\n",
      "Epoch 6 / 30 [Train] MSE Loss: 11.3172, [Test] MSE Loss: 10.8723, running time : 13.9 sec \n",
      "--------------------\n",
      "Epoch 7 / 30 [Train] MSE Loss: 10.3771, [Test] MSE Loss: 9.9154, running time : 16.2 sec \n",
      "--------------------\n",
      "Epoch 8 / 30 [Train] MSE Loss: 9.3202, [Test] MSE Loss: 8.7381, running time : 18.5 sec \n",
      "--------------------\n",
      "Epoch 9 / 30 [Train] MSE Loss: 8.1909, [Test] MSE Loss: 7.6694, running time : 20.9 sec \n",
      "--------------------\n",
      "Epoch 10 / 30 [Train] MSE Loss: 7.0636, [Test] MSE Loss: 6.4305, running time : 23.2 sec \n",
      "--------------------\n",
      "Epoch 11 / 30 [Train] MSE Loss: 5.8762, [Test] MSE Loss: 5.3476, running time : 25.5 sec \n",
      "--------------------\n",
      "Epoch 12 / 30 [Train] MSE Loss: 4.7407, [Test] MSE Loss: 4.2987, running time : 27.8 sec \n",
      "--------------------\n",
      "Epoch 13 / 30 [Train] MSE Loss: 3.7132, [Test] MSE Loss: 3.1423, running time : 30.2 sec \n",
      "--------------------\n",
      "Epoch 14 / 30 [Train] MSE Loss: 2.8021, [Test] MSE Loss: 2.5078, running time : 32.6 sec \n",
      "--------------------\n",
      "Epoch 15 / 30 [Train] MSE Loss: 2.0494, [Test] MSE Loss: 2.2855, running time : 34.9 sec \n",
      "--------------------\n",
      "Epoch 16 / 30 [Train] MSE Loss: 1.4732, [Test] MSE Loss: 1.7639, running time : 37.3 sec \n",
      "--------------------\n",
      "Epoch 17 / 30 [Train] MSE Loss: 1.0778, [Test] MSE Loss: 1.3057, running time : 39.8 sec \n",
      "--------------------\n",
      "Epoch 18 / 30 [Train] MSE Loss: 0.8146, [Test] MSE Loss: 1.2663, running time : 42.1 sec \n",
      "--------------------\n",
      "Epoch 19 / 30 [Train] MSE Loss: 0.7671, [Test] MSE Loss: 1.2162, running time : 44.4 sec \n",
      "--------------------\n",
      "Epoch 20 / 30 [Train] MSE Loss: 0.6109, [Test] MSE Loss: 1.2438, running time : 46.7 sec \n",
      "--------------------\n",
      "Epoch 21 / 30 [Train] MSE Loss: 0.6633, [Test] MSE Loss: 1.2309, running time : 49.0 sec \n",
      "--------------------\n",
      "Epoch 22 / 30 [Train] MSE Loss: 0.6216, [Test] MSE Loss: 1.4099, running time : 51.3 sec \n",
      "--------------------\n",
      "Epoch 23 / 30 [Train] MSE Loss: 0.6317, [Test] MSE Loss: 1.2543, running time : 53.7 sec \n",
      "--------------------\n",
      "Epoch 24 / 30 [Train] MSE Loss: 0.5655, [Test] MSE Loss: 1.2653, running time : 56.0 sec \n",
      "--------------------\n",
      "Epoch 25 / 30 [Train] MSE Loss: 0.5753, [Test] MSE Loss: 1.2161, running time : 58.3 sec \n",
      "--------------------\n",
      "Epoch 26 / 30 [Train] MSE Loss: 0.5271, [Test] MSE Loss: 1.2933, running time : 60.6 sec \n",
      "--------------------\n",
      "Epoch 27 / 30 [Train] MSE Loss: 0.4640, [Test] MSE Loss: 1.2537, running time : 62.9 sec \n",
      "--------------------\n",
      "Epoch 28 / 30 [Train] MSE Loss: 0.4835, [Test] MSE Loss: 1.1639, running time : 65.2 sec \n",
      "--------------------\n",
      "Epoch 29 / 30 [Train] MSE Loss: 0.4008, [Test] MSE Loss: 1.2491, running time : 67.4 sec \n",
      "--------------------\n",
      "Epoch 30 / 30 [Train] MSE Loss: 0.4362, [Test] MSE Loss: 1.1878, running time : 69.7 sec \n",
      "--------------------\n",
      "total elapsed time is 69.7sec\n"
     ]
    }
   ],
   "source": [
    " model = train_model(model, train_loader, test_loader, num_epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
