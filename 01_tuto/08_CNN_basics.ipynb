{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Convolution Block Basics\n",
    "##### (ref. https://github.com/buomsoo-kim/PyTorch-learners-tutorial)\n",
    "##### (ref. https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolution\n",
    "\n",
    "\n",
    "Preprocessing image with filter applied to image. The filter applied to image is called kernel\n",
    "That is, we could extract feature with convolutional kenel \n",
    "\n",
    "- Notions for convolution layer\n",
    "    - **Kernel Size** – the size of the filter.\n",
    "    - **Kernel Type** – the values of the actual filter. Some examples include identity, edge detection, and sharpen\n",
    "    - **Stride** – the rate at which the kernel passes over the input image. A stride of 2 moves the kernel in 2-pixel increments\n",
    "    - **Padding** – we can add layers of 0s to the outside of the image in order to make sure that the kernel properly passes over the edges of the image\n",
    "    - **Output Layers** – how many different kernels are applied to the image\n",
    "\n",
    "\n",
    "- How to calculate output size of convolution operation\n",
    "  <br> \n",
    "*(W - F + 2P)/S + 1* <br>\n",
    "  - *W*: input size\n",
    "  - *F*: kernel size\n",
    "  - *P*: padding \n",
    "  - *S*: stride\n",
    "  \n",
    "![alt text](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)\n",
    "<br>\n",
    "<br>\n",
    "![alt_text](https://qph.fs.quoracdn.net/main-qimg-af9899617c2beedbc89c036e3b8a9e78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1. 1-d Convolution layer\n",
    "- ```torch.nn.Conv1d()```: 1D convolution\n",
    "  - Parameters\n",
    "      - ```in_channels``` : size of input channel\n",
    "      - ```out_channels``` : size of output channel\n",
    "      - ```kernel_size``` : kennel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 10])\n"
     ]
    }
   ],
   "source": [
    "# case 1 - kernel size = 1\n",
    "conv1d = nn.Conv1d(16, 32, kernel_size = 1)\n",
    "\n",
    "x = torch.ones(128, 16, 10)   # input: batch_size = 128, num_filters = 16, seq_length = 10\n",
    "print(conv1d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)\n",
    "# output length = (10 - 1 + 2*0)/1 + 1 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 13])\n"
     ]
    }
   ],
   "source": [
    "# case 2 - kernel size = 2, stride = 1\n",
    "conv1d = nn.Conv1d(16, 32, kernel_size = 2, padding = 2)\n",
    "\n",
    "x = torch.ones(128, 16, 10)   # input: batch_size = 128, num_filters = 16, seq_length = 10\n",
    "print(conv1d(x).size())\n",
    "# output length = (10 - 2 + 2*2)/1 + 1 = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "# weight for conv layer is [# of filter, # of each filters channel, kernel size]\n",
    "print(conv1d.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2. 2-d Convolution layer\n",
    "- ```torch.nn.Conv2d()```: 1D convolution\n",
    "  - Parameters\n",
    "      - ```in_channels``` : size of input channel\n",
    "      - ```out_channels``` : size of output channel\n",
    "      - ```kernel_size``` : kennel size\n",
    "      - ```stride``` : size of stride\n",
    "      - ```padding``` : size of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 10, 10])\n",
      "torch.Size([128, 32, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# case 1 - kernel size = 1\n",
    "conv2d = nn.Conv2d(16, 32, kernel_size = 1)  # if kernel size is integer, width and height are equal (i.e., square kernel) \n",
    "\n",
    "x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n",
    "print(conv2d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)\n",
    "#output length = (10 - 1 + 2*0)/1 + 1 = 10\n",
    "\n",
    "conv2d = nn.Conv2d(16, 32, kernel_size = (1, 1))  # same as kernel size = 1\n",
    "\n",
    "x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n",
    "print(conv2d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 5, 5])\n",
      "torch.Size([128, 32, 5, 5])\n",
      "torch.Size([128, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# case 1 - kernel size = 1 and stride = 2\n",
    "conv2d = nn.Conv2d(16, 32, kernel_size = 1, stride=2)  # if kernel size is integer, width and height are equal (i.e., square kernel) \n",
    "\n",
    "x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n",
    "print(conv2d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)\n",
    "#output length = (10 - 1 + 2*0)/2 + 1 = 5.5 -> 5\n",
    "\n",
    "# case 2 - kernel size = 2 and stride = 2\n",
    "conv2d = nn.Conv2d(16, 32, kernel_size = 2, stride = 2) \n",
    "\n",
    "x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n",
    "print(conv2d(x).size()) \n",
    "\n",
    "# case 2 - kernel size = 2 and stride = 2\n",
    "conv2d = nn.Conv2d(16, 32, kernel_size = 2, stride = 2, padding=2) \n",
    "\n",
    "x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n",
    "print(conv2d(x).size()) \n",
    "#output length = (10 - 1 + 2*2)/2 + 1 = 7.5 -> 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# weight for conv layer is [# of filter, # of each filters channel, kernel size(w), kernel_size(h)]\n",
    "print(conv2d.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Max Pooling\n",
    "\n",
    "Pooling the highest values of extracted feature from Convolutional filter  \n",
    "Depending on size of pooling, we could down-sampling the feature set  \n",
    "Advantage for using max-pooling is to lower variance which may not helpful.   \n",
    "That is, it extracts the sharpest features of an image\n",
    "Also, It is helpful to lower computation of neural network\n",
    "\n",
    "\n",
    "<br>\n",
    "  \n",
    "![alt text](https://blog.algorithmia.com/wp-content/uploads/2018/03/word-image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```torch.nn.MaxPool2d()```: 2D convolution\n",
    "  - Parameters\n",
    "    - **kernel** : size of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 5, 5])\n",
      "torch.Size([128, 32, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# case 1 - kernel size = 1 and stride = 2\n",
    "conv2d = nn.Conv2d(16, 32, kernel_size = 1, stride=2)  # if kernel size is integer, width and height are equal (i.e., square kernel) \n",
    "maxpool2d = nn.MaxPool2d(2)\n",
    "\n",
    "x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n",
    "print(conv2d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)\n",
    "#output length = (10 - 1 + 2*0)/2 + 1 = 5.5 -> 5\n",
    "print(maxpool2d(conv2d(x)).size()) \n",
    "# size is to be half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaxPool2d' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-495c95a07e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# weight for conv layer is [# of filter, # of each filters channel, kernel size(w), kernel_size(h)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxpool2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/py_libs/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaxPool2d' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "# weight for conv layer is [# of filter, # of each filters channel, kernel size(w), kernel_size(h)]\n",
    "print(maxpool2d.weight.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
